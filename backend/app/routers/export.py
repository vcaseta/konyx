from fastapi import APIRouter, HTTPException, UploadFile, File, Form
from fastapi.responses import FileResponse, StreamingResponse
from app.core.persistence import load_data, save_data
from datetime import datetime
import pandas as pd
import io, os, json, time, re

# --- GPT ---
import openai
from dotenv import load_dotenv
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

router = APIRouter(prefix="/export", tags=["export"])

# ============================================================
# üì° STREAM DE PROGRESO (SSE)
# ============================================================
progress_log: list[str] = []

def log_step(text: str):
    """A√±ade un mensaje al stream de progreso (y al log de servidor)."""
    progress_log.append(text)
    print("üì°", text)

def progress_stream():
    """Genera eventos SSE con los pasos acumulados."""
    for step in progress_log:
        yield f"data: {json.dumps({'step': step})}\n\n"
        time.sleep(0.6)  # ritmo agradable de lectura
    yield "event: end\ndata: {}\n\n"

@router.get("/progress")
async def export_progress():
    """Endpoint SSE: el frontend se conecta aqu√≠ para leer el progreso en vivo."""
    return StreamingResponse(progress_stream(), media_type="text/event-stream")


# ============================================================
# üîé VALIDACI√ìN DE FORMATOS
# ============================================================
def detectar_formato_sesiones(df: pd.DataFrame) -> str:
    cols = [c.strip().lower() for c in df.columns]
    # ‚ÄúEholo‚Äù gen√©rico: presencia de campos clave
    if all(any(ec in c for c in cols) for ec in ["fecha", "iva", "total"]):
        return "Eholo"
    # ‚ÄúGestor√≠a‚Äù aproximado: bastantes coincidencias de cabeceras t√≠picas
    gestor_cols = [
        "fecha factura", "numero factura", "nombre", "nif",
        "concepto", "importe base", "iva", "irpf", "total"
    ]
    matches = sum(any(gc in c for c in cols) for gc in gestor_cols)
    if matches >= 6:
        return "Gestor√≠a"
    return "Desconocido"

def validar_contactos(df: pd.DataFrame):
    cols = [c.strip().lower() for c in df.columns]
    # Basta con que exista al menos un identificador razonable
    if not any(x in cols for x in ["nombre", "nif", "cif", "email"]):
        raise HTTPException(
            status_code=400,
            detail="El archivo de contactos no tiene columnas v√°lidas (nombre, NIF/CIF o email)."
        )

def normalizar_columnas(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [re.sub(r"\s+", " ", c.strip().lower()) for c in df.columns]
    # Normalizaciones frecuentes
    if "cif" in df.columns and "nif" not in df.columns:
        df.rename(columns={"cif": "nif"}, inplace=True)
    if "razon social" in df.columns and "nombre" not in df.columns:
        df.rename(columns={"razon social": "nombre"}, inplace=True)
    return df


# ============================================================
# ü§ñ GPT: AN√ÅLISIS Y CORRECCI√ìN
# ============================================================
def _extract_json(text: str) -> dict | None:
    try:
        start = text.find("{")
        end = text.rfind("}") + 1
        if start == -1 or end == 0:
            return None
        return json.loads(text[start:end])
    except Exception:
        return None

def analizar_tablas_gpt(df_ses: pd.DataFrame, df_con: pd.DataFrame) -> dict | None:
    """
    Pide a GPT un mapeo de columnas y clave de uni√≥n.
    Devuelve dict con: {"join_key": "...", "fields": {...}, "csv_columns": [...]}
    """
    try:
        muestras_ses = df_ses.head(10).fillna("").to_dict(orient="records")
        muestras_con = df_con.head(10).fillna("").to_dict(orient="records")

        prompt = f"""
Analiza estas dos tablas (muestras):

[Sesiones]
{muestras_ses}

[Contactos]
{muestras_con}

Devuelve SOLO JSON con esta forma:
{{
  "join_key": "nif | nombre | email",
  "fields": {{
    "nombre": ["posibles encabezados en sesiones o contactos"],
    "nif": ["..."],
    "importe": ["total", "importe base", "..."],
    "fecha": ["fecha", "fecha factura", "..."],
    "concepto": ["concepto", "descripcion", "..."],
    "iva": ["iva", "..."],
    "irpf": ["irpf", "..."],
    "total": ["total", "..."]
  }},
  "csv_columns": ["fecha", "nombre", "nif", "concepto", "importe base", "iva", "irpf", "total", "empresa", "proyecto", "cuenta contable", "usuario export"]
}}
No a√±adas explicaciones fuera del JSON.
"""

        completion = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en conciliaci√≥n contable y an√°lisis tabular."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )
        raw = completion.choices[0].message.content
        data = _extract_json(raw)
        return data
    except Exception as e:
        print("‚ùå Error en analizar_tablas_gpt:", e)
        return None

def detectar_cambios(df_original: pd.DataFrame, df_corregido: pd.DataFrame, prefix: str = "") -> list[str]:
    """Compara dos DataFrames y devuelve una lista textual de diferencias."""
    cambios: list[str] = []
    # Igualamos columnas si fuera necesario
    inter_cols = [c for c in df_original.columns if c in df_corregido.columns]
    ori = df_original[inter_cols].reset_index(drop=True)
    cor = df_corregido[inter_cols].reset_index(drop=True)

    # Garantizamos mismo length para comparar (hasta el m√≠nimo)
    length = min(len(ori), len(cor))
    for i in range(length):
        for col in inter_cols:
            o = "" if pd.isna(ori.at[i, col]) else str(ori.at[i, col])
            c = "" if pd.isna(cor.at[i, col]) else str(cor.at[i, col])
            if o.strip() != c.strip():
                # Algunos mensajes bonitos seg√∫n columna
                emoji = "‚úèÔ∏è"
                if col in ("fecha", "fecha factura"): emoji = "üìÖ"
                if col in ("nif", "cif"): emoji = "üßæ"
                if col in ("importe", "total", "importe base", "iva", "irpf"): emoji = "üí∂"
                cambios.append(f"{emoji} {prefix}{col}: '{o}' ‚Üí '{c}' (fila {i+1})")
    return cambios

def corregir_datos_con_gpt(df_ses: pd.DataFrame, df_con: pd.DataFrame):
    """
    Env√≠a muestras a GPT para que proponga correcciones/normalizaciones.
    Devuelve (df_ses_corregido, df_con_corregido) y emite logs de cambios.
    """
    try:
        muestras_ses = df_ses.head(15).fillna("").to_dict(orient="records")
        muestras_con = df_con.head(15).fillna("").to_dict(orient="records")

        prompt = f"""
Corrige y completa los datos de estas tablas. Reglas:
- Normaliza fechas a formato dd/mm/yyyy.
- Normaliza NIF/CIF (quitar espacios/guiones, may√∫sculas).
- Arregla nombres si hay coincidencias evidentes.
- Convierte importes a n√∫mero decimal (punto como separador).
- Rellena valores vac√≠os cuando se puedan inferir.
- Mant√©n los nombres de campos originales cuando sea razonable.
- Devuelve SOLO JSON con:
{{
  "sesiones": [ {{... filas corregidas ...}} ],
  "contactos": [ {{... filas corregidas ...}} ],
  "correcciones": ["texto humano breve por cambio detectado (opcional)"]
}}
No incluyas explicaciones fuera del JSON.
Facturas (sesiones): {muestras_ses}
Contactos: {muestras_con}
"""

        completion = openai.chat.completions.create(
            model="gpt-4o-mini",
            messages=[
                {"role": "system", "content": "Eres un experto en limpieza y conciliaci√≥n de datos contables."},
                {"role": "user", "content": prompt}
            ],
            temperature=0.2,
        )

        raw = completion.choices[0].message.content
        data = _extract_json(raw)
        if not data or "sesiones" not in data or "contactos" not in data:
            log_step("‚ö†Ô∏è GPT no devolvi√≥ un JSON v√°lido de correcciones. Se mantienen datos originales.")
            return df_ses, df_con

        df_ses_corr = pd.DataFrame(data["sesiones"])
        df_con_corr = pd.DataFrame(data["contactos"])

        # Emisi√≥n de cambios detectados (comparativa local)
        cambios = detectar_cambios(df_ses, df_ses_corr, prefix="Sesi√≥n ")
        cambios += detectar_cambios(df_con, df_con_corr, prefix="Contacto ")

        # A√±adimos correcciones textuales opcionales del propio GPT
        if "correcciones" in data and isinstance(data["correcciones"], list):
            cambios += [f"ü§ñ {c}" for c in data["correcciones"]]

        # Publicamos (limitamos para no saturar)
        for msg in cambios[:120]:
            log_step(msg)

        log_step(f"‚úÖ GPT complet√≥ {len(cambios)} correcciones/normalizaciones.")
        return df_ses_corr, df_con_corr

    except Exception as e:
        log_step(f"‚ö†Ô∏è Error corrigiendo datos con GPT: {e}")
        return df_ses, df_con


# ============================================================
# üöÄ ENDPOINT PRINCIPAL
# ============================================================
@router.post("/start")
async def start_export(
    formatoImport: str = Form(...),
    formatoExport: str = Form(...),
    empresa: str = Form(...),
    fechaFactura: str = Form(...),
    proyecto: str = Form(...),
    cuenta: str = Form(...),
    usuario: str = Form(...),
    ficheroSesiones: UploadFile = File(...),
    ficheroContactos: UploadFile = File(...),
):
    """
    Flujo real:
    1) Lee Excel
    2) Valida formatos
    3) Corrige/normaliza con GPT (y emite cambios)
    4) Analiza mapeo con GPT para renombrar/ligar columnas
    5) Merge por join_key
    6) A√±ade campos extra (empresa, proyecto, cuenta, fecha)
    7) Genera CSV y actualiza m√©tricas
    """
    # reset del stream
    progress_log.clear()

    try:
        log_step("üîÑ Iniciando exportaci√≥n...")
        log_step("üì• Leyendo ficheros Excel...")

        # 1) Leer ficheros
        ses_bytes = await ficheroSesiones.read()
        con_bytes = await ficheroContactos.read()
        df_ses = pd.read_excel(io.BytesIO(ses_bytes))
        df_con = pd.read_excel(io.BytesIO(con_bytes))
        log_step("‚úÖ Ficheros cargados.")

        # 2) Validar formatos
        log_step("üîé Validando formato de sesiones...")
        tipo_detectado = detectar_formato_sesiones(df_ses)
        if tipo_detectado == "Desconocido":
            raise HTTPException(status_code=400, detail="El archivo de sesiones no es Eholo ni Gestor√≠a.")
        log_step(f"üß© Sesiones ‚Üí formato detectado: {tipo_detectado}")

        log_step("üîé Validando formato de contactos...")
        validar_contactos(df_con)
        log_step("‚úÖ Contactos v√°lidos.")

        # Normalizar cabeceras frecuentes
        df_ses = normalizar_columnas(df_ses)
        df_con = normalizar_columnas(df_con)

        # 3) Correcci√≥n / normalizaci√≥n sem√°ntica con GPT
        log_step("ü§ñ Analizando y corrigiendo datos con GPT...")
        df_ses, df_con = corregir_datos_con_gpt(df_ses, df_con)
        log_step("‚úÖ Datos corregidos por GPT (si fue posible).")

        # 4) An√°lisis de mapeo con GPT
        log_step("üß† Obteniendo mapeo de columnas y clave de uni√≥n (GPT)...")
        mapeo = analizar_tablas_gpt(df_ses, df_con)
        if not mapeo:
            log_step("‚ö†Ô∏è GPT no devolvi√≥ mapeo v√°lido. Intentando uni√≥n local por NIF o nombre.")
            join_key = "nif" if ("nif" in df_ses.columns and "nif" in df_con.columns) else "nombre"
            fields = {}
            csv_cols = []
        else:
            join_key = mapeo.get("join_key", "nif")
            fields = mapeo.get("fields", {})
            csv_cols = mapeo.get("csv_columns", [])
            # Renombrado de columnas seg√∫n mapeo
            for campo, posibles in fields.items():
                if not isinstance(posibles, list): 
                    continue
                for cand in posibles:
                    c = cand.strip().lower()
                    if c in df_ses.columns and campo not in df_ses.columns:
                        df_ses.rename(columns={c: campo}, inplace=True)
                    if c in df_con.columns and campo not in df_con.columns:
                        df_con.rename(columns={c: campo}, inplace=True)
            log_step(f"üóÇÔ∏è Columnas renombradas seg√∫n GPT. Uni√≥n por '{join_key}'.")

        # Asegurar normalizaci√≥n clave para join
        if "nif" in df_ses.columns: df_ses["nif"] = df_ses["nif"].astype(str).str.strip().str.upper()
        if "nif" in df_con.columns: df_con["nif"] = df_con["nif"].astype(str).str.strip().str.upper()
        if "nombre" in df_ses.columns: df_ses["nombre"] = df_ses["nombre"].astype(str).str.strip().str.lower()
        if "nombre" in df_con.columns: df_con["nombre"] = df_con["nombre"].astype(str).str.strip().str.lower()

        # 5) Merge
        if join_key not in df_ses.columns or join_key not in df_con.columns:
            log_step(f"‚ö†Ô∏è La clave '{join_key}' no existe en ambas tablas. Usando fallback.")
            join_key = "nif" if ("nif" in df_ses.columns and "nif" in df_con.columns) else "nombre"

        log_step(f"üîó Conciliando por '{join_key}'...")
        merged = pd.merge(df_ses, df_con, how="left", on=join_key)
        log_step("‚úÖ Conciliaci√≥n realizada.")

        # 6) Campos extra de interfaz
        merged["fecha factura"] = fechaFactura
        merged["empresa"] = empresa
        merged["proyecto"] = proyecto
        merged["cuenta contable"] = cuenta
        merged["usuario export"] = usuario
        log_step("üßæ Campos de contexto aplicados (empresa, proyecto, cuenta, fecha).")

        # 7) CSV
        os.makedirs("./app/exports", exist_ok=True)
        filename = f"export_{empresa.replace(' ', '_')}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
        filepath = f"./app/exports/{filename}"

        # columnas finales ‚Äî si GPT propuso, usar; si no, best-effort
        columnas_csv = csv_cols or [
            c for c in [
                "fecha factura", "numero factura", "nombre", "nif", "concepto",
                "importe base", "iva", "irpf", "total", "empresa",
                "proyecto", "cuenta contable", "usuario export"
            ] if c in merged.columns
        ]
        merged.to_csv(filepath, index=False, encoding="utf-8-sig", columns=columnas_csv)
        log_step("üíæ CSV generado correctamente.")

        # 8) M√©tricas
        data = load_data()
        data["ultimoExport"] = datetime.now().strftime("%d/%m/%Y")
        data["totalExportaciones"] = data.get("totalExportaciones", 0) + 1
        save_data(data)
        log_step("üìà Estad√≠sticas actualizadas.")
        log_step("‚úÖ Exportaci√≥n finalizada.")

        return {
            "message": "Exportaci√≥n completada correctamente",
            "archivo_generado": filename,
            "ultimoExport": data["ultimoExport"],
            "totalExportaciones": data["totalExportaciones"]
        }

    except HTTPException:
        # ya formateado
        raise
    except Exception as e:
        data = load_data()
        data["totalExportacionesFallidas"] = data.get("totalExportacionesFallidas", 0) + 1
        save_data(data)
        log_step(f"‚ùå Error en exportaci√≥n: {e}")
        raise HTTPException(status_code=400, detail=f"Error procesando exportaci√≥n: {e}")


# ============================================================
# üì• DESCARGA DEL CSV
# ============================================================
@router.get("/download/{filename}")
async def download_csv(filename: str):
    path = f"./app/exports/{filename}"
    if not os.path.exists(path):
        raise HTTPException(status_code=404, detail="Archivo no encontrado")
    return FileResponse(path, media_type="text/csv", filename=filename)
